{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinmaysultanpuri/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Inspecting google/gemma-3-1b-it\n",
      "============================================================\n",
      "\n",
      "üìã MODEL CONFIGURATION:\n",
      "  Model type: gemma3_text\n",
      "  Architecture: ['Gemma3ForCausalLM']\n",
      "  Hidden size: 1152\n",
      "  Number of layers: 26\n",
      "  Number of attention heads: 4\n",
      "  Intermediate size: 6912\n",
      "  Vocabulary size: 262144\n",
      "  Max position embeddings: 32768\n",
      "\n",
      "üî§ TOKENIZER DETAILS:\n",
      "  Tokenizer type: GemmaTokenizerFast\n",
      "  Vocabulary size: 262145\n",
      "  Special tokens: {'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'boi_token': '<start_of_image>', 'eoi_token': '<end_of_image>', 'image_token': '<image_soft_token>'}\n",
      "\n",
      "üß™ TOKENIZATION TESTS:\n",
      "  'I think the emotional state of this user is' ‚Üí ['I', '‚ñÅthink', '‚ñÅthe', '‚ñÅemotional', '‚ñÅstate', '‚ñÅof', '‚ñÅthis', '‚ñÅuser', '‚ñÅis'] ‚Üí [236777, 1751, 506, 13690, 1883, 529, 672, 2430, 563]\n",
      "  'is' ‚Üí ['is'] ‚Üí [511]\n",
      "  ' is' ‚Üí ['‚ñÅis'] ‚Üí [563]\n",
      "  'state is' ‚Üí ['state', '‚ñÅis'] ‚Üí [3255, 563]\n",
      "\n",
      "üèóÔ∏è  LOADING MODEL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['output_hidden_states']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ Model loaded successfully\n",
      "  Device: cpu\n",
      "  Data type: torch.float16\n",
      "\n",
      "üìä MODEL SIZE:\n",
      "  Total parameters: 999,885,952\n",
      "  Trainable parameters: 999,885,952\n",
      "  Model size: ~2.00 GB (fp16)\n",
      "\n",
      "üß† HIDDEN STATE EXTRACTION TEST:\n",
      "  Test input: 'I think the emotional state of this user is'\n",
      "  Input shape: torch.Size([1, 10])\n",
      "  Input tokens: [2, 236777, 1751, 506, 13690, 1883, 529, 672, 2430, 563]\n",
      "  Number of layers (including embedding): 27\n",
      "  Layer 0: torch.Size([1, 10, 1152]) ‚Üí Last token vector: torch.Size([1152])\n",
      "  Layer 1: torch.Size([1, 10, 1152]) ‚Üí Last token vector: torch.Size([1152])\n",
      "  Layer 2: torch.Size([1, 10, 1152]) ‚Üí Last token vector: torch.Size([1152])\n",
      "  Layer 3: torch.Size([1, 10, 1152]) ‚Üí Last token vector: torch.Size([1152])\n",
      "  Layer 4: torch.Size([1, 10, 1152]) ‚Üí Last token vector: torch.Size([1152])\n",
      "  Layer 5: torch.Size([1, 10, 1152]) ‚Üí Last token vector: torch.Size([1152])\n",
      "  Layer 6: torch.Size([1, 10, 1152]) ‚Üí Last token vector: torch.Size([1152])\n",
      "  Layer 7: torch.Size([1, 10, 1152]) ‚Üí Last token vector: torch.Size([1152])\n",
      "  Layer 8: torch.Size([1, 10, 1152]) ‚Üí Last token vector: torch.Size([1152])\n",
      "  Layer 9: torch.Size([1, 10, 1152]) ‚Üí Last token vector: torch.Size([1152])\n",
      "  Layer 10: torch.Size([1, 10, 1152]) ‚Üí Last token vector: torch.Size([1152])\n",
      "  Layer 11: torch.Size([1, 10, 1152]) ‚Üí Last token vector: torch.Size([1152])\n",
      "  Layer 12: torch.Size([1, 10, 1152]) ‚Üí Last token vector: torch.Size([1152])\n",
      "  Layer 13: torch.Size([1, 10, 1152]) ‚Üí Last token vector: torch.Size([1152])\n",
      "  Layer 14: torch.Size([1, 10, 1152]) ‚Üí Last token vector: torch.Size([1152])\n",
      "  Layer 15: torch.Size([1, 10, 1152]) ‚Üí Last token vector: torch.Size([1152])\n",
      "  Layer 16: torch.Size([1, 10, 1152]) ‚Üí Last token vector: torch.Size([1152])\n",
      "  Layer 17: torch.Size([1, 10, 1152]) ‚Üí Last token vector: torch.Size([1152])\n",
      "  Layer 18: torch.Size([1, 10, 1152]) ‚Üí Last token vector: torch.Size([1152])\n",
      "  Layer 19: torch.Size([1, 10, 1152]) ‚Üí Last token vector: torch.Size([1152])\n",
      "  Layer 20: torch.Size([1, 10, 1152]) ‚Üí Last token vector: torch.Size([1152])\n",
      "  Layer 21: torch.Size([1, 10, 1152]) ‚Üí Last token vector: torch.Size([1152])\n",
      "  Layer 22: torch.Size([1, 10, 1152]) ‚Üí Last token vector: torch.Size([1152])\n",
      "  Layer 23: torch.Size([1, 10, 1152]) ‚Üí Last token vector: torch.Size([1152])\n",
      "  Layer 24: torch.Size([1, 10, 1152]) ‚Üí Last token vector: torch.Size([1152])\n",
      "  Layer 25: torch.Size([1, 10, 1152]) ‚Üí Last token vector: torch.Size([1152])\n",
      "  Layer 26: torch.Size([1, 10, 1152]) ‚Üí Last token vector: torch.Size([1152])\n",
      "\n",
      "üéØ PROBING EXPERIMENT DETAILS:\n",
      "  Hidden dimension for LogisticRegression input: 1152\n",
      "  Number of probe layers to train: 27\n",
      "  Expected feature vector shape per sample: (1152,)\n",
      "  Memory per hidden state vector: 4608 bytes (fp32)\n",
      "  Memory per sample (all layers): 124416 bytes\n",
      "  Estimated memory for 440 samples: 52.21 MB\n",
      "\n",
      "============================================================\n",
      "üìã LOGISTIC REGRESSION PROBE SPECIFICATIONS\n",
      "============================================================\n",
      "Input feature dimension: 1152\n",
      "Number of probes to train: 27\n",
      "Output classes: 4 (Happy, Sad, Angry, Neutral)\n",
      "Probe type: LogisticRegression(multi_class='ovr')\n",
      "Expected X.shape per layer: (n_samples, 1152)\n",
      "Expected y.shape: (n_samples,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Gemma-3-1B Model Architecture Inspector\n",
    "Extracts detailed model specifications for probing experiments\n",
    "\"\"\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig\n",
    "import torch\n",
    "\n",
    "def inspect_gemma_model():\n",
    "    model_name = \"google/gemma-3-1b-it\"\n",
    "    \n",
    "    print(f\"üîç Inspecting {model_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load configuration\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    print(\"\\nüìã MODEL CONFIGURATION:\")\n",
    "    print(f\"  Model type: {config.model_type}\")\n",
    "    print(f\"  Architecture: {config.architectures}\")\n",
    "    print(f\"  Hidden size: {config.hidden_size}\")\n",
    "    print(f\"  Number of layers: {config.num_hidden_layers}\")\n",
    "    print(f\"  Number of attention heads: {config.num_attention_heads}\")\n",
    "    print(f\"  Intermediate size: {config.intermediate_size}\")\n",
    "    print(f\"  Vocabulary size: {config.vocab_size}\")\n",
    "    print(f\"  Max position embeddings: {config.max_position_embeddings}\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    print(f\"\\nüî§ TOKENIZER DETAILS:\")\n",
    "    print(f\"  Tokenizer type: {type(tokenizer).__name__}\")\n",
    "    print(f\"  Vocabulary size: {len(tokenizer)}\")\n",
    "    print(f\"  Special tokens: {tokenizer.special_tokens_map}\")\n",
    "    \n",
    "    # Test tokenization of key phrases\n",
    "    test_phrases = [\n",
    "        \"I think the emotional state of this user is\",\n",
    "        \"is\",\n",
    "        \" is\",\n",
    "        \"state is\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nüß™ TOKENIZATION TESTS:\")\n",
    "    for phrase in test_phrases:\n",
    "        tokens = tokenizer.tokenize(phrase)\n",
    "        token_ids = tokenizer.encode(phrase, add_special_tokens=False)\n",
    "        print(f\"  '{phrase}' ‚Üí {tokens} ‚Üí {token_ids}\")\n",
    "    \n",
    "    # Load model and inspect architecture\n",
    "    print(f\"\\nüèóÔ∏è  LOADING MODEL...\")\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name, \n",
    "            output_hidden_states=True,\n",
    "            torch_dtype=torch.float16,  # Use half precision to save memory\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        model.eval()\n",
    "        \n",
    "        print(f\"  ‚úÖ Model loaded successfully\")\n",
    "        print(f\"  Device: {next(model.parameters()).device}\")\n",
    "        print(f\"  Data type: {next(model.parameters()).dtype}\")\n",
    "        \n",
    "        # Get model size info\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        print(f\"\\nüìä MODEL SIZE:\")\n",
    "        print(f\"  Total parameters: {total_params:,}\")\n",
    "        print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "        print(f\"  Model size: ~{total_params * 2 / 1e9:.2f} GB (fp16)\")\n",
    "        \n",
    "        # Test hidden state extraction\n",
    "        test_text = \"I think the emotional state of this user is\"\n",
    "        print(f\"\\nüß† HIDDEN STATE EXTRACTION TEST:\")\n",
    "        print(f\"  Test input: '{test_text}'\")\n",
    "        \n",
    "        inputs = tokenizer(test_text, return_tensors=\"pt\")\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        print(f\"  Input shape: {inputs['input_ids'].shape}\")\n",
    "        print(f\"  Input tokens: {inputs['input_ids'][0].tolist()}\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        hidden_states = outputs.hidden_states\n",
    "        print(f\"  Number of layers (including embedding): {len(hidden_states)}\")\n",
    "        \n",
    "        for i, layer_states in enumerate(hidden_states):\n",
    "            print(f\"  Layer {i}: {layer_states.shape} ‚Üí Last token vector: {layer_states[0, -1, :].shape}\")\n",
    "        \n",
    "        # Specific details for your probing experiment\n",
    "        print(f\"\\nüéØ PROBING EXPERIMENT DETAILS:\")\n",
    "        print(f\"  Hidden dimension for LogisticRegression input: {config.hidden_size}\")\n",
    "        print(f\"  Number of probe layers to train: {len(hidden_states)}\")\n",
    "        print(f\"  Expected feature vector shape per sample: ({config.hidden_size},)\")\n",
    "        print(f\"  Memory per hidden state vector: {config.hidden_size * 4} bytes (fp32)\")\n",
    "        print(f\"  Memory per sample (all layers): {len(hidden_states) * config.hidden_size * 4} bytes\")\n",
    "        \n",
    "        # Estimate memory for your dataset\n",
    "        n_samples = 440  # From your dataset\n",
    "        total_memory_mb = (n_samples * len(hidden_states) * config.hidden_size * 4) / (1024 * 1024)\n",
    "        print(f\"  Estimated memory for 440 samples: {total_memory_mb:.2f} MB\")\n",
    "        \n",
    "        return {\n",
    "            'hidden_size': config.hidden_size,\n",
    "            'num_layers': len(hidden_states),\n",
    "            'vocab_size': config.vocab_size,\n",
    "            'model_config': config,\n",
    "            'tokenizer': tokenizer\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error loading model: {e}\")\n",
    "        print(f\"  Falling back to config-only inspection\")\n",
    "        \n",
    "        return {\n",
    "            'hidden_size': config.hidden_size,\n",
    "            'num_layers': config.num_hidden_layers + 1,  # +1 for embedding layer\n",
    "            'vocab_size': config.vocab_size,\n",
    "            'model_config': config,\n",
    "            'tokenizer': tokenizer\n",
    "        }\n",
    "\n",
    "def get_probe_input_specs():\n",
    "    \"\"\"Returns the exact specifications for your logistic regression probes\"\"\"\n",
    "    details = inspect_gemma_model()\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(\"üìã LOGISTIC REGRESSION PROBE SPECIFICATIONS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Input feature dimension: {details['hidden_size']}\")\n",
    "    print(f\"Number of probes to train: {details['num_layers']}\")\n",
    "    print(f\"Output classes: 4 (Happy, Sad, Angry, Neutral)\")\n",
    "    print(f\"Probe type: LogisticRegression(multi_class='ovr')\")\n",
    "    print(f\"Expected X.shape per layer: (n_samples, {details['hidden_size']})\")\n",
    "    print(f\"Expected y.shape: (n_samples,)\")\n",
    "    \n",
    "    return details\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_details = get_probe_input_specs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
