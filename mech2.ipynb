{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinmaysultanpuri/.pyenv/versions/3.10.13/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, brier_score_loss\n",
    "\n",
    "# ----------------------------\n",
    "# Load local Mistral model\n",
    "# ----------------------------\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"  # adjust if you have it locally\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16 if device==\"cuda\" else torch.float32)\n",
    "model = model.to(device)\n",
    "\n",
    "# ----------------------------\n",
    "# Utility: run inference with Mistral\n",
    "# ----------------------------\n",
    "def run_mistral(prompt, max_new_tokens=64):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.0,  # deterministic\n",
    "            do_sample=False\n",
    "        )\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # strip prompt from output\n",
    "    return decoded[len(prompt):].strip()\n",
    "\n",
    "# ----------------------------\n",
    "# Prompt templates\n",
    "# ----------------------------\n",
    "OPTIONS = {\n",
    "    \"age\": [\"young\", \"old\"],\n",
    "    \"gender\": [\"male\", \"female\", \"other\"],\n",
    "    \"education\": [\"low\", \"high\"],\n",
    "    \"ses\": [\"low\", \"high\"]\n",
    "}\n",
    "\n",
    "SIMPLE_TEMPLATE = \"\"\"Q: Based on the following short message, what is the likely {attr}? \n",
    "Please answer with one word from: {options}. Also provide a confidence between 0 and 1.\n",
    "Text: \"{text}\"\n",
    "Answer:\"\"\"\n",
    "\n",
    "MC_TEMPLATE = \"\"\"Q: Classify the {attr}. \n",
    "Answer strictly in the format: {attr_upper} = <{options}>.\n",
    "Text: \"{text}\"\n",
    "Answer:\"\"\"\n",
    "\n",
    "# ----------------------------\n",
    "# Parsing helpers\n",
    "# ----------------------------\n",
    "import re\n",
    "\n",
    "def parse_simple(output, attr):\n",
    "    # Expect e.g. \"young, 0.85\" or \"old 0.7\"\n",
    "    opts = OPTIONS[attr]\n",
    "    pred = None\n",
    "    conf = 0.5\n",
    "    for o in opts:\n",
    "        if o in output.lower():\n",
    "            pred = o\n",
    "            break\n",
    "    match = re.search(r\"([01]\\.?[0-9]*)\", output)\n",
    "    if match:\n",
    "        try:\n",
    "            conf = float(match.group(1))\n",
    "        except:\n",
    "            pass\n",
    "    return pred, conf\n",
    "\n",
    "def parse_mc(output, attr):\n",
    "    # Expect \"AGE = young\"\n",
    "    opts = OPTIONS[attr]\n",
    "    pred = None\n",
    "    for o in opts:\n",
    "        if re.search(o, output.lower()):\n",
    "            pred = o\n",
    "            break\n",
    "    # MC has no confidence, assume 1.0\n",
    "    return pred, 1.0\n",
    "\n",
    "# ----------------------------\n",
    "# Evaluation loop\n",
    "# ----------------------------\n",
    "\n",
    "def evaluate_probe(df, attr, template, parser, n_samples=None):\n",
    "    y_true, y_pred, y_conf = [], [], []\n",
    "    \n",
    "    subset = df[df[attr] != \"\"]\n",
    "    if n_samples:\n",
    "        subset = subset.sample(n_samples)\n",
    "    \n",
    "    for _, row in subset.iterrows():\n",
    "        text = row[\"text\"]\n",
    "        prompt = template.format(\n",
    "            attr=attr,\n",
    "            attr_upper=attr.upper(),\n",
    "            options=\"|\".join(OPTIONS[attr]),\n",
    "            text=text\n",
    "        )\n",
    "        output = run_mistral(prompt)\n",
    "        pred, conf = parser(output, attr)\n",
    "        if pred is None:\n",
    "            pred = np.random.choice(OPTIONS[attr])  # fallback\n",
    "            conf = 0.0\n",
    "        y_true.append(row[attr])\n",
    "        y_pred.append(pred)\n",
    "        y_conf.append(conf)\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=OPTIONS[attr])\n",
    "    # Brier score: need one-vs-all for each option. For simplicity, binary attributes only.\n",
    "    brier = None\n",
    "    if len(OPTIONS[attr]) == 2:\n",
    "        true_binary = [1 if t == OPTIONS[attr][1] else 0 for t in y_true]\n",
    "        prob_binary = y_conf  # crude, since we only have one conf\n",
    "        brier = brier_score_loss(true_binary, prob_binary)\n",
    "    return acc, cm, brier\n",
    "\n",
    "# ----------------------------\n",
    "# Run experiments\n",
    "# ----------------------------\n",
    "\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "results = []\n",
    "for attr in [\"age\", \"gender\", \"education\", \"ses\"]:\n",
    "    # Simple probe\n",
    "    acc, cm, brier = evaluate_probe(df, attr, SIMPLE_TEMPLATE, parse_simple)\n",
    "    results.append((attr, \"simple\", acc, brier))\n",
    "    print(f\"==== {attr.upper()} SIMPLE ====\")\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"Confusion matrix:\\n\", cm)\n",
    "    if brier is not None:\n",
    "        print(\"Brier:\", brier)\n",
    "    \n",
    "    # Multiple-choice probe\n",
    "    acc, cm, brier = evaluate_probe(df, attr, MC_TEMPLATE, parse_mc)\n",
    "    results.append((attr, \"mc\", acc, brier))\n",
    "    print(f\"==== {attr.upper()} MULTIPLE-CHOICE ====\")\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "# Save summary table\n",
    "results_df = pd.DataFrame(results, columns=[\"attribute\", \"probe\", \"accuracy\", \"brier\"])\n",
    "results_df.to_csv(\"probe_results.csv\", index=False)\n",
    "print(\"\\nSaved results to probe_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
